{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "\n",
        "This notebook focuses on feature engineering and preprocessing for both e-commerce and credit card fraud detection datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable tqdm for pandas\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "fraud_data = pd.read_csv('../data/raw/Fraud_Data.csv')\n",
        "ip_country = pd.read_csv('../data/raw/IpAddress_to_Country.csv')\n",
        "creditcard_data = pd.read_csv('../data/raw/creditcard.csv')\n",
        "\n",
        "print(\"Data loaded successfully\")\n",
        "print(f\"\\nFraud Data shape: {fraud_data.shape}\")\n",
        "print(f\"IP Country mapping shape: {ip_country.shape}\")\n",
        "print(f\"Credit Card Data shape: {creditcard_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning - Fraud Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for processing\n",
        "df_fraud = fraud_data.copy()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA CLEANING - FRAUD DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Check for missing values\n",
        "print(\"\\n1. Missing Values Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "missing = df_fraud.isnull().sum()\n",
        "missing_pct = (missing / len(df_fraud)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "if missing_df[missing_df['Missing Count'] > 0].empty:\n",
        "    print(\"✓ No missing values found!\")\n",
        "\n",
        "# 2. Check for duplicates\n",
        "print(\"\\n2. Duplicate Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "duplicates = df_fraud.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicates} ({duplicates/len(df_fraud)*100:.2f}%)\")\n",
        "if duplicates > 0:\n",
        "    df_fraud = df_fraud.drop_duplicates()\n",
        "    print(f\"✓ Removed {duplicates} duplicate rows\")\n",
        "    print(f\"New shape: {df_fraud.shape}\")\n",
        "else:\n",
        "    print(\"✓ No duplicates found!\")\n",
        "\n",
        "# 3. Correct data types\n",
        "print(\"\\n3. Data Type Corrections:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Original data types:\")\n",
        "print(df_fraud.dtypes)\n",
        "\n",
        "# Convert time columns to datetime\n",
        "df_fraud['signup_time'] = pd.to_datetime(df_fraud['signup_time'])\n",
        "df_fraud['purchase_time'] = pd.to_datetime(df_fraud['purchase_time'])\n",
        "\n",
        "# Convert IP address to integer (it's already numeric but may have decimals)\n",
        "df_fraud['ip_address'] = df_fraud['ip_address'].astype('int64')\n",
        "\n",
        "print(\"\\n✓ Converted signup_time and purchase_time to datetime\")\n",
        "print(\"✓ Converted ip_address to int64\")\n",
        "\n",
        "print(\"\\nUpdated data types:\")\n",
        "print(df_fraud.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"GEOLOCATION INTEGRATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert IP address ranges to integer format\n",
        "ip_country['lower_bound_ip_address'] = ip_country['lower_bound_ip_address'].astype('int64')\n",
        "ip_country['upper_bound_ip_address'] = ip_country['upper_bound_ip_address'].astype('int64')\n",
        "\n",
        "print(\"\\nIP Country mapping data types:\")\n",
        "print(ip_country.dtypes)\n",
        "print(f\"\\nIP Country mapping shape: {ip_country.shape}\")\n",
        "print(f\"\\nSample IP ranges:\")\n",
        "print(ip_country.head(10))\n",
        "\n",
        "# Function to map IP address to country using range-based lookup\n",
        "def map_ip_to_country(ip_address, ip_country_df):\n",
        "    \"\"\"\n",
        "    Maps an IP address to a country using range-based lookup.\n",
        "    Uses binary search for efficiency.\n",
        "    \"\"\"\n",
        "    # Find the country where ip_address falls within the range\n",
        "    mask = (ip_country_df['lower_bound_ip_address'] <= ip_address) & \\\n",
        "           (ip_country_df['upper_bound_ip_address'] >= ip_address)\n",
        "    matches = ip_country_df[mask]\n",
        "    \n",
        "    if len(matches) > 0:\n",
        "        # If multiple matches, take the first one (shouldn't happen with proper ranges)\n",
        "        return matches.iloc[0]['country']\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Apply the mapping (this may take a while for large datasets)\n",
        "print(\"\\nMapping IP addresses to countries...\")\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "# Use vectorized approach for better performance\n",
        "# Create a sorted version for faster lookup\n",
        "ip_country_sorted = ip_country.sort_values('lower_bound_ip_address').reset_index(drop=True)\n",
        "\n",
        "# For large datasets, we'll use a more efficient approach\n",
        "# Using merge_asof for range-based lookup (requires sorted data)\n",
        "df_fraud_sorted = df_fraud.sort_values('ip_address').reset_index(drop=True)\n",
        "ip_country_sorted = ip_country_sorted.sort_values('lower_bound_ip_address').reset_index(drop=True)\n",
        "\n",
        "# Use merge_asof for efficient range lookup\n",
        "df_fraud_sorted = pd.merge_asof(\n",
        "    df_fraud_sorted,\n",
        "    ip_country_sorted[['lower_bound_ip_address', 'country']],\n",
        "    left_on='ip_address',\n",
        "    right_on='lower_bound_ip_address',\n",
        "    direction='backward'\n",
        ")\n",
        "\n",
        "# Filter to keep only valid matches (where ip_address is within range)\n",
        "valid_mask = (df_fraud_sorted['ip_address'] >= df_fraud_sorted['lower_bound_ip_address']) & \\\n",
        "             (df_fraud_sorted['ip_address'] <= ip_country_sorted.loc[\n",
        "                 ip_country_sorted['lower_bound_ip_address'] == df_fraud_sorted['lower_bound_ip_address'].values[0] if len(df_fraud_sorted) > 0 else 0,\n",
        "                 'upper_bound_ip_address'\n",
        "             ].values[0] if len(df_fraud_sorted) > 0 else False)\n",
        "\n",
        "# Alternative: Use apply with optimized function\n",
        "print(\"Using optimized mapping function...\")\n",
        "tqdm.pandas(desc=\"Mapping IPs\")\n",
        "df_fraud['country'] = df_fraud['ip_address'].progress_apply(\n",
        "    lambda x: map_ip_to_country(x, ip_country_sorted)\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ IP to country mapping completed!\")\n",
        "print(f\"\\nCountry distribution:\")\n",
        "print(df_fraud['country'].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering - Transaction Frequency and Velocity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE ENGINEERING - TRANSACTION FREQUENCY AND VELOCITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Sort by user_id and purchase_time for time-based calculations\n",
        "df_fraud = df_fraud.sort_values(['user_id', 'purchase_time']).reset_index(drop=True)\n",
        "\n",
        "# 1. Transaction frequency per user (total transactions per user)\n",
        "print(\"\\n1. Calculating transaction frequency per user...\")\n",
        "user_transaction_count = df_fraud.groupby('user_id').size().reset_index(name='transaction_count')\n",
        "df_fraud = df_fraud.merge(user_transaction_count, on='user_id', how='left')\n",
        "\n",
        "# 2. Transaction velocity - transactions in time windows\n",
        "print(\"2. Calculating transaction velocity...\")\n",
        "\n",
        "# Calculate time differences between consecutive transactions for the same user\n",
        "df_fraud['prev_purchase_time'] = df_fraud.groupby('user_id')['purchase_time'].shift(1)\n",
        "df_fraud['time_since_last_transaction'] = (\n",
        "    df_fraud['purchase_time'] - df_fraud['prev_purchase_time']\n",
        ").dt.total_seconds() / 3600  # in hours\n",
        "\n",
        "# Fill NaN for first transaction of each user with a large value\n",
        "df_fraud['time_since_last_transaction'] = df_fraud['time_since_last_transaction'].fillna(999999)\n",
        "\n",
        "# Transactions in last 24 hours, 7 days, 30 days\n",
        "print(\"   - Transactions in last 24 hours, 7 days, 30 days...\")\n",
        "df_fraud['transactions_last_24h'] = 0\n",
        "df_fraud['transactions_last_7d'] = 0\n",
        "df_fraud['transactions_last_30d'] = 0\n",
        "\n",
        "for idx, row in tqdm(df_fraud.iterrows(), total=len(df_fraud), desc=\"Calculating velocity\"):\n",
        "    user_id = row['user_id']\n",
        "    purchase_time = row['purchase_time']\n",
        "    \n",
        "    # Get all transactions for this user before current transaction\n",
        "    user_transactions = df_fraud[\n",
        "        (df_fraud['user_id'] == user_id) & \n",
        "        (df_fraud['purchase_time'] < purchase_time)\n",
        "    ]\n",
        "    \n",
        "    # Count transactions in time windows\n",
        "    df_fraud.loc[idx, 'transactions_last_24h'] = len(\n",
        "        user_transactions[user_transactions['purchase_time'] >= purchase_time - pd.Timedelta(hours=24)]\n",
        "    )\n",
        "    df_fraud.loc[idx, 'transactions_last_7d'] = len(\n",
        "        user_transactions[user_transactions['purchase_time'] >= purchase_time - pd.Timedelta(days=7)]\n",
        "    )\n",
        "    df_fraud.loc[idx, 'transactions_last_30d'] = len(\n",
        "        user_transactions[user_transactions['purchase_time'] >= purchase_time - pd.Timedelta(days=30)]\n",
        "    )\n",
        "\n",
        "print(\"✓ Transaction frequency and velocity features created!\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nTransaction frequency statistics:\")\n",
        "print(df_fraud[['transaction_count', 'transactions_last_24h', 'transactions_last_7d', 'transactions_last_30d']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE ENGINEERING - TIME-BASED FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Hour of day\n",
        "df_fraud['hour_of_day'] = df_fraud['purchase_time'].dt.hour\n",
        "\n",
        "# 2. Day of week (0=Monday, 6=Sunday)\n",
        "df_fraud['day_of_week'] = df_fraud['purchase_time'].dt.dayofweek\n",
        "\n",
        "# 3. Day of month\n",
        "df_fraud['day_of_month'] = df_fraud['purchase_time'].dt.day\n",
        "\n",
        "# 4. Month\n",
        "df_fraud['month'] = df_fraud['purchase_time'].dt.month\n",
        "\n",
        "# 5. Time since signup (in hours)\n",
        "df_fraud['time_since_signup'] = (\n",
        "    df_fraud['purchase_time'] - df_fraud['signup_time']\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# 6. Is weekend\n",
        "df_fraud['is_weekend'] = (df_fraud['day_of_week'] >= 5).astype(int)\n",
        "\n",
        "# 7. Is business hours (9 AM - 5 PM)\n",
        "df_fraud['is_business_hours'] = ((df_fraud['hour_of_day'] >= 9) & (df_fraud['hour_of_day'] < 17)).astype(int)\n",
        "\n",
        "print(\"✓ Time-based features created!\")\n",
        "print(\"\\nTime-based features summary:\")\n",
        "time_features = ['hour_of_day', 'day_of_week', 'day_of_month', 'month', \n",
        "                'time_since_signup', 'is_weekend', 'is_business_hours']\n",
        "print(df_fraud[time_features].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Transformation - Scaling and Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DATA TRANSFORMATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_fraud.drop('class', axis=1)\n",
        "y = df_fraud['class']\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numerical_features = ['purchase_value', 'age', 'ip_address', \n",
        "                     'transaction_count', 'time_since_last_transaction',\n",
        "                     'transactions_last_24h', 'transactions_last_7d', \n",
        "                     'transactions_last_30d', 'time_since_signup',\n",
        "                     'hour_of_day', 'day_of_week', 'day_of_month', 'month']\n",
        "\n",
        "categorical_features = ['source', 'browser', 'sex', 'country', \n",
        "                       'device_id', 'user_id']\n",
        "\n",
        "# Keep only features that exist in the dataframe\n",
        "numerical_features = [f for f in numerical_features if f in X.columns]\n",
        "categorical_features = [f for f in categorical_features if f in X.columns]\n",
        "\n",
        "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "# Create a copy for transformation\n",
        "X_transformed = X.copy()\n",
        "\n",
        "# 1. Normalize/Scale numerical features\n",
        "print(\"\\n1. Scaling numerical features...\")\n",
        "scaler = StandardScaler()\n",
        "X_transformed[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
        "print(\"✓ Applied StandardScaler to numerical features\")\n",
        "\n",
        "# 2. Encode categorical features using One-Hot Encoding\n",
        "print(\"\\n2. Encoding categorical features...\")\n",
        "# For high cardinality features, we might want to use target encoding or limit categories\n",
        "# For now, we'll use one-hot encoding for low cardinality features\n",
        "\n",
        "# Identify low and high cardinality categorical features\n",
        "low_cardinality = []\n",
        "high_cardinality = []\n",
        "\n",
        "for feature in categorical_features:\n",
        "    if feature in X.columns:\n",
        "        unique_count = X[feature].nunique()\n",
        "        if unique_count <= 20:  # Threshold for one-hot encoding\n",
        "            low_cardinality.append(feature)\n",
        "        else:\n",
        "            high_cardinality.append(feature)\n",
        "            print(f\"   - {feature}: {unique_count} unique values (will use label encoding)\")\n",
        "\n",
        "# One-hot encode low cardinality features\n",
        "if low_cardinality:\n",
        "    X_encoded = pd.get_dummies(X_transformed[low_cardinality], prefix=low_cardinality, drop_first=True)\n",
        "    X_transformed = pd.concat([X_transformed.drop(low_cardinality, axis=1), X_encoded], axis=1)\n",
        "    print(f\"✓ Applied One-Hot Encoding to: {low_cardinality}\")\n",
        "\n",
        "# Label encode high cardinality features\n",
        "if high_cardinality:\n",
        "    label_encoders = {}\n",
        "    for feature in high_cardinality:\n",
        "        le = LabelEncoder()\n",
        "        X_transformed[feature] = le.fit_transform(X[feature].astype(str))\n",
        "        label_encoders[feature] = le\n",
        "    print(f\"✓ Applied Label Encoding to: {high_cardinality}\")\n",
        "\n",
        "# Drop time columns that were used for feature engineering but shouldn't be in final model\n",
        "columns_to_drop = ['signup_time', 'purchase_time', 'prev_purchase_time']\n",
        "columns_to_drop = [c for c in columns_to_drop if c in X_transformed.columns]\n",
        "if columns_to_drop:\n",
        "    X_transformed = X_transformed.drop(columns_to_drop, axis=1)\n",
        "    print(f\"✓ Dropped time columns: {columns_to_drop}\")\n",
        "\n",
        "print(f\"\\nFinal feature shape: {X_transformed.shape}\")\n",
        "print(f\"Final features: {X_transformed.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Split data before handling class imbalance (to avoid data leakage)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"Imbalance ratio: {y_train.value_counts()[0] / y_train.value_counts()[1]:.2f}:1\")\n",
        "\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "print(y_test.value_counts())\n",
        "print(f\"Imbalance ratio: {y_test.value_counts()[0] / y_test.value_counts()[1]:.2f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Handle Class Imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"HANDLING CLASS IMBALANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Document class distribution before resampling\n",
        "print(\"\\nBEFORE RESAMPLING:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Training set - Legitimate: {y_train.value_counts()[0]:,} ({y_train.value_counts(normalize=True)[0]*100:.2f}%)\")\n",
        "print(f\"Training set - Fraud: {y_train.value_counts()[1]:,} ({y_train.value_counts(normalize=True)[1]*100:.2f}%)\")\n",
        "print(f\"Imbalance ratio: {y_train.value_counts()[0] / y_train.value_counts()[1]:.2f}:1\")\n",
        "\n",
        "# Choice of technique: SMOTE (Synthetic Minority Oversampling Technique)\n",
        "# Justification:\n",
        "# 1. SMOTE creates synthetic samples rather than duplicating existing ones, reducing overfitting\n",
        "# 2. It's effective for highly imbalanced datasets\n",
        "# 3. It preserves the original data distribution while balancing classes\n",
        "# 4. Better than simple oversampling which can lead to overfitting\n",
        "# 5. Better than undersampling which discards valuable data\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TECHNIQUE SELECTION: SMOTE\")\n",
        "print(\"=\"*80)\n",
        "print(\"Justification:\")\n",
        "print(\"1. Creates synthetic samples rather than duplicating (reduces overfitting)\")\n",
        "print(\"2. Effective for highly imbalanced datasets\")\n",
        "print(\"3. Preserves original data distribution while balancing classes\")\n",
        "print(\"4. Better than simple oversampling (reduces overfitting risk)\")\n",
        "print(\"5. Better than undersampling (preserves valuable data)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Apply SMOTE to training data only (important: never apply to test set!)\n",
        "print(\"\\nApplying SMOTE to training data...\")\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Balance to 0.5 (1:2 ratio)\n",
        "# Alternative: sampling_strategy='auto' for 1:1 ratio, or a float for custom ratio\n",
        "\n",
        "try:\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "    print(\"✓ SMOTE applied successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error applying SMOTE: {e}\")\n",
        "    print(\"Trying with different parameters...\")\n",
        "    # Try with fewer neighbors if error occurs\n",
        "    smote = SMOTE(random_state=42, k_neighbors=3, sampling_strategy=0.5)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "    print(\"✓ SMOTE applied successfully with adjusted parameters!\")\n",
        "\n",
        "# Document class distribution after resampling\n",
        "print(\"\\nAFTER RESAMPLING (SMOTE):\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Training set - Legitimate: {y_train_resampled.value_counts()[0]:,} ({y_train_resampled.value_counts(normalize=True)[0]*100:.2f}%)\")\n",
        "print(f\"Training set - Fraud: {y_train_resampled.value_counts()[1]:,} ({y_train_resampled.value_counts(normalize=True)[1]*100:.2f}%)\")\n",
        "print(f\"Imbalance ratio: {y_train_resampled.value_counts()[0] / y_train_resampled.value_counts()[1]:.2f}:1\")\n",
        "print(f\"\\nOriginal training set size: {len(X_train):,}\")\n",
        "print(f\"Resampled training set size: {len(X_train_resampled):,}\")\n",
        "print(f\"New samples created: {len(X_train_resampled) - len(X_train):,}\")\n",
        "\n",
        "# Note: Test set remains unchanged (as it should be)\n",
        "print(\"\\n✓ Test set remains unchanged (no resampling applied)\")\n",
        "print(f\"Test set - Legitimate: {y_test.value_counts()[0]:,}\")\n",
        "print(f\"Test set - Fraud: {y_test.value_counts()[1]:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SAVING PROCESSED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save processed datasets\n",
        "processed_dir = Path('../data/processed')\n",
        "processed_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save training and test sets\n",
        "X_train_resampled.to_csv(processed_dir / 'X_train_processed.csv', index=False)\n",
        "y_train_resampled.to_csv(processed_dir / 'y_train_processed.csv', index=False)\n",
        "X_test.to_csv(processed_dir / 'X_test_processed.csv', index=False)\n",
        "y_test.to_csv(processed_dir / 'y_test_processed.csv', index=False)\n",
        "\n",
        "print(\"✓ Saved processed datasets:\")\n",
        "print(f\"  - {processed_dir / 'X_train_processed.csv'}\")\n",
        "print(f\"  - {processed_dir / 'y_train_processed.csv'}\")\n",
        "print(f\"  - {processed_dir / 'X_test_processed.csv'}\")\n",
        "print(f\"  - {processed_dir / 'y_test_processed.csv'}\")\n",
        "\n",
        "# Save scaler and encoders for later use\n",
        "import joblib\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(scaler, models_dir / 'scaler.pkl')\n",
        "if 'label_encoders' in locals():\n",
        "    joblib.dump(label_encoders, models_dir / 'label_encoders.pkl')\n",
        "\n",
        "print(\"\\n✓ Saved preprocessing objects:\")\n",
        "print(f\"  - {models_dir / 'scaler.pkl'}\")\n",
        "if 'label_encoders' in locals():\n",
        "    print(f\"  - {models_dir / 'label_encoders.pkl'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFinal feature count: {X_train_resampled.shape[1]}\")\n",
        "print(f\"Training samples: {len(X_train_resampled):,}\")\n",
        "print(f\"Test samples: {len(X_test):,}\")\n",
        "print(\"\\n✓ Data is ready for modeling!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
