{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Model Building and Training\n",
        "\n",
        "This notebook implements:\n",
        "1. Data Preparation\n",
        "2. Baseline Model (Logistic Regression)\n",
        "3. Ensemble Models (XGBoost, Random Forest, LightGBM)\n",
        "4. Cross-Validation\n",
        "5. Model Comparison and Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# XGBoost and LightGBM\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "Load the processed training and test datasets that were prepared in the feature engineering step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "data_dir = Path('../data/processed')\n",
        "\n",
        "print(\"Loading processed datasets...\")\n",
        "X_train = pd.read_csv(data_dir / 'X_train_processed.csv')\n",
        "y_train = pd.read_csv(data_dir / 'y_train_processed.csv')\n",
        "X_test = pd.read_csv(data_dir / 'X_test_processed.csv')\n",
        "y_test = pd.read_csv(data_dir / 'y_test_processed.csv')\n",
        "\n",
        "# Extract target variable (handle both single column and multi-column cases)\n",
        "if y_train.shape[1] == 1:\n",
        "    y_train = y_train.iloc[:, 0]\n",
        "else:\n",
        "    y_train = y_train['class']\n",
        "\n",
        "if y_test.shape[1] == 1:\n",
        "    y_test = y_test.iloc[:, 0]\n",
        "else:\n",
        "    y_test = y_test['class']\n",
        "\n",
        "print(f\"\\nTraining set shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Test set shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTraining set class percentages:\")\n",
        "print(y_train.value_counts(normalize=True) * 100)\n",
        "\n",
        "print(f\"\\nTest set class distribution:\")\n",
        "print(y_test.value_counts())\n",
        "print(f\"\\nTest set class percentages:\")\n",
        "print(y_test.value_counts(normalize=True) * 100)\n",
        "\n",
        "# Verify no missing values\n",
        "print(f\"\\nMissing values in X_train: {X_train.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in y_train: {y_train.isnull().sum()}\")\n",
        "print(f\"Missing values in y_test: {y_test.isnull().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Evaluation Metrics Function\n",
        "\n",
        "Define a function to evaluate models using appropriate metrics for imbalanced data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, y_pred_proba=None, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Evaluate model performance with multiple metrics suitable for imbalanced data.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        True labels\n",
        "    y_pred : array-like\n",
        "        Predicted labels\n",
        "    y_pred_proba : array-like, optional\n",
        "        Predicted probabilities for positive class\n",
        "    model_name : str\n",
        "        Name of the model for display\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing all metrics\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Basic metrics\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['f1_score'] = f1_score(y_true, y_pred, zero_division=0)\n",
        "    \n",
        "    # Probability-based metrics (if probabilities provided)\n",
        "    if y_pred_proba is not None:\n",
        "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
        "        metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} - Evaluation Metrics\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
        "    print(f\"F1-Score:  {metrics['f1_score']:.4f}\")\n",
        "    if y_pred_proba is not None:\n",
        "        print(f\"ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
        "        print(f\"PR-AUC:    {metrics['pr_auc']:.4f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\"):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Legitimate', 'Fraud'],\n",
        "                yticklabels=['Legitimate', 'Fraud'])\n",
        "    plt.title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return cm\n",
        "\n",
        "def plot_roc_pr_curves(y_true, y_pred_proba, model_name=\"Model\"):\n",
        "    \"\"\"Plot ROC and Precision-Recall curves\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
        "                label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "    axes[0].set_xlim([0.0, 1.0])\n",
        "    axes[0].set_ylim([0.0, 1.05])\n",
        "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "    axes[0].set_title(f'{model_name} - ROC Curve', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(loc=\"lower right\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
        "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
        "    \n",
        "    axes[1].plot(recall, precision, color='darkorange', lw=2,\n",
        "                label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "    axes[1].set_xlim([0.0, 1.0])\n",
        "    axes[1].set_ylim([0.0, 1.05])\n",
        "    axes[1].set_xlabel('Recall', fontsize=12)\n",
        "    axes[1].set_ylabel('Precision', fontsize=12)\n",
        "    axes[1].set_title(f'{model_name} - Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(loc=\"lower left\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Evaluation functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression baseline model\n",
        "print(\"Training Logistic Regression baseline model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use class_weight='balanced' to handle imbalance\n",
        "lr_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',  # Automatically adjust class weights\n",
        "    solver='lbfgs'  # Good for small to medium datasets\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "lr_metrics = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")\n",
        "lr_cm = plot_confusion_matrix(y_test, y_pred_lr, \"Logistic Regression\")\n",
        "plot_roc_pr_curves(y_test, y_pred_proba_lr, \"Logistic Regression\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Legitimate', 'Fraud']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ensemble Models\n",
        "\n",
        "Train ensemble models: Random Forest, XGBoost, and LightGBM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest model\n",
        "print(\"Training Random Forest model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "rf_metrics = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest\")\n",
        "rf_cm = plot_confusion_matrix(y_test, y_pred_rf, \"Random Forest\")\n",
        "plot_roc_pr_curves(y_test, y_pred_proba_rf, \"Random Forest\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Legitimate', 'Fraud']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 XGBoost Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost model\n",
        "print(\"Training XGBoost model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Handle imbalance\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    n_jobs=-1,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "xgb_metrics = evaluate_model(y_test, y_pred_xgb, y_pred_proba_xgb, \"XGBoost\")\n",
        "xgb_cm = plot_confusion_matrix(y_test, y_pred_xgb, \"XGBoost\")\n",
        "plot_roc_pr_curves(y_test, y_pred_proba_xgb, \"XGBoost\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=['Legitimate', 'Fraud']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 LightGBM Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LightGBM model\n",
        "print(\"Training LightGBM model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "lgb_metrics = evaluate_model(y_test, y_pred_lgb, y_pred_proba_lgb, \"LightGBM\")\n",
        "lgb_cm = plot_confusion_matrix(y_test, y_pred_lgb, \"LightGBM\")\n",
        "plot_roc_pr_curves(y_test, y_pred_proba_lgb, \"LightGBM\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lgb, target_names=['Legitimate', 'Fraud']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Tuning\n",
        "\n",
        "Perform basic hyperparameter tuning for the best performing ensemble model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# We'll tune XGBoost as it typically performs well for fraud detection\n",
        "print(\"Performing hyperparameter tuning for XGBoost...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.8, 0.9]\n",
        "}\n",
        "\n",
        "# Use PR-AUC as scoring metric (better for imbalanced data)\n",
        "xgb_tuned = GridSearchCV(\n",
        "    estimator=xgb.XGBClassifier(\n",
        "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        n_jobs=-1,\n",
        "        verbosity=0\n",
        "    ),\n",
        "    param_grid=param_grid,\n",
        "    scoring='average_precision',  # PR-AUC\n",
        "    cv=3,  # 3-fold CV for faster tuning\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Fitting GridSearchCV (this may take a few minutes)...\")\n",
        "xgb_tuned.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {xgb_tuned.best_params_}\")\n",
        "print(f\"Best CV score (PR-AUC): {xgb_tuned.best_score_:.4f}\")\n",
        "\n",
        "# Make predictions with tuned model\n",
        "y_pred_xgb_tuned = xgb_tuned.predict(X_test)\n",
        "y_pred_proba_xgb_tuned = xgb_tuned.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate tuned model\n",
        "xgb_tuned_metrics = evaluate_model(y_test, y_pred_xgb_tuned, y_pred_proba_xgb_tuned, \"XGBoost (Tuned)\")\n",
        "xgb_tuned_cm = plot_confusion_matrix(y_test, y_pred_xgb_tuned, \"XGBoost (Tuned)\")\n",
        "plot_roc_pr_curves(y_test, y_pred_proba_xgb_tuned, \"XGBoost (Tuned)\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb_tuned, target_names=['Legitimate', 'Fraud']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define cross-validation function\n",
        "def perform_cross_validation(model, X, y, model_name=\"Model\", cv_folds=5):\n",
        "    \"\"\"\n",
        "    Perform stratified k-fold cross-validation and report mean and std of metrics.\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Store results for each fold\n",
        "    cv_results = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1_score': [],\n",
        "        'roc_auc': [],\n",
        "        'pr_auc': []\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} - {cv_folds}-Fold Cross-Validation Results\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    fold = 1\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_fold_train, y_fold_train)\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_fold_val)\n",
        "        y_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        cv_results['accuracy'].append(accuracy_score(y_fold_val, y_pred))\n",
        "        cv_results['precision'].append(precision_score(y_fold_val, y_pred, zero_division=0))\n",
        "        cv_results['recall'].append(recall_score(y_fold_val, y_pred, zero_division=0))\n",
        "        cv_results['f1_score'].append(f1_score(y_fold_val, y_pred, zero_division=0))\n",
        "        cv_results['roc_auc'].append(roc_auc_score(y_fold_val, y_pred_proba))\n",
        "        cv_results['pr_auc'].append(average_precision_score(y_fold_val, y_pred_proba))\n",
        "        \n",
        "        print(f\"Fold {fold}: PR-AUC = {cv_results['pr_auc'][-1]:.4f}, \"\n",
        "              f\"F1 = {cv_results['f1_score'][-1]:.4f}, \"\n",
        "              f\"Recall = {cv_results['recall'][-1]:.4f}\")\n",
        "        fold += 1\n",
        "    \n",
        "    # Calculate mean and std\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Cross-Validation Summary (Mean ± Std)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Accuracy:  {np.mean(cv_results['accuracy']):.4f} ± {np.std(cv_results['accuracy']):.4f}\")\n",
        "    print(f\"Precision: {np.mean(cv_results['precision']):.4f} ± {np.std(cv_results['precision']):.4f}\")\n",
        "    print(f\"Recall:    {np.mean(cv_results['recall']):.4f} ± {np.std(cv_results['recall']):.4f}\")\n",
        "    print(f\"F1-Score:  {np.mean(cv_results['f1_score']):.4f} ± {np.std(cv_results['f1_score']):.4f}\")\n",
        "    print(f\"ROC-AUC:   {np.mean(cv_results['roc_auc']):.4f} ± {np.std(cv_results['roc_auc']):.4f}\")\n",
        "    print(f\"PR-AUC:    {np.mean(cv_results['pr_auc']):.4f} ± {np.std(cv_results['pr_auc']):.4f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return cv_results\n",
        "\n",
        "# Perform cross-validation for all models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CROSS-VALIDATION FOR ALL MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Logistic Regression CV\n",
        "lr_cv = perform_cross_validation(\n",
        "    LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced', solver='lbfgs'),\n",
        "    X_train, y_train, \"Logistic Regression\", cv_folds=5\n",
        ")\n",
        "\n",
        "# Random Forest CV\n",
        "rf_cv = perform_cross_validation(\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=15, min_samples_split=10,\n",
        "                          min_samples_leaf=5, class_weight='balanced', random_state=42, n_jobs=-1),\n",
        "    X_train, y_train, \"Random Forest\", cv_folds=5\n",
        ")\n",
        "\n",
        "# XGBoost CV\n",
        "xgb_cv = perform_cross_validation(\n",
        "    xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                     scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
        "                     random_state=42, eval_metric='logloss', n_jobs=-1, verbosity=0),\n",
        "    X_train, y_train, \"XGBoost\", cv_folds=5\n",
        ")\n",
        "\n",
        "# LightGBM CV\n",
        "lgb_cv = perform_cross_validation(\n",
        "    lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                     class_weight='balanced', random_state=42, n_jobs=-1, verbose=-1),\n",
        "    X_train, y_train, \"LightGBM\", cv_folds=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM', 'XGBoost (Tuned)'],\n",
        "    'PR-AUC (Test)': [\n",
        "        lr_metrics['pr_auc'],\n",
        "        rf_metrics['pr_auc'],\n",
        "        xgb_metrics['pr_auc'],\n",
        "        lgb_metrics['pr_auc'],\n",
        "        xgb_tuned_metrics['pr_auc']\n",
        "    ],\n",
        "    'F1-Score (Test)': [\n",
        "        lr_metrics['f1_score'],\n",
        "        rf_metrics['f1_score'],\n",
        "        xgb_metrics['f1_score'],\n",
        "        lgb_metrics['f1_score'],\n",
        "        xgb_tuned_metrics['f1_score']\n",
        "    ],\n",
        "    'Recall (Test)': [\n",
        "        lr_metrics['recall'],\n",
        "        rf_metrics['recall'],\n",
        "        xgb_metrics['recall'],\n",
        "        lgb_metrics['recall'],\n",
        "        xgb_tuned_metrics['recall']\n",
        "    ],\n",
        "    'Precision (Test)': [\n",
        "        lr_metrics['precision'],\n",
        "        rf_metrics['precision'],\n",
        "        xgb_metrics['precision'],\n",
        "        lgb_metrics['precision'],\n",
        "        xgb_tuned_metrics['precision']\n",
        "    ],\n",
        "    'PR-AUC (CV Mean)': [\n",
        "        np.mean(lr_cv['pr_auc']),\n",
        "        np.mean(rf_cv['pr_auc']),\n",
        "        np.mean(xgb_cv['pr_auc']),\n",
        "        np.mean(lgb_cv['pr_auc']),\n",
        "        xgb_tuned.best_score_ if 'xgb_tuned' in locals() else np.mean(xgb_cv['pr_auc'])\n",
        "    ],\n",
        "    'PR-AUC (CV Std)': [\n",
        "        np.std(lr_cv['pr_auc']),\n",
        "        np.std(rf_cv['pr_auc']),\n",
        "        np.std(xgb_cv['pr_auc']),\n",
        "        np.std(lgb_cv['pr_auc']),\n",
        "        0  # GridSearchCV doesn't provide std directly\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('PR-AUC (Test)', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# PR-AUC comparison\n",
        "axes[0, 0].barh(comparison_df['Model'], comparison_df['PR-AUC (Test)'], color='steelblue')\n",
        "axes[0, 0].set_xlabel('PR-AUC Score', fontsize=12)\n",
        "axes[0, 0].set_title('PR-AUC Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# F1-Score comparison\n",
        "axes[0, 1].barh(comparison_df['Model'], comparison_df['F1-Score (Test)'], color='darkgreen')\n",
        "axes[0, 1].set_xlabel('F1-Score', fontsize=12)\n",
        "axes[0, 1].set_title('F1-Score Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Recall comparison\n",
        "axes[1, 0].barh(comparison_df['Model'], comparison_df['Recall (Test)'], color='crimson')\n",
        "axes[1, 0].set_xlabel('Recall', fontsize=12)\n",
        "axes[1, 0].set_title('Recall Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Precision comparison\n",
        "axes[1, 1].barh(comparison_df['Model'], comparison_df['Precision (Test)'], color='orange')\n",
        "axes[1, 1].set_xlabel('Precision', fontsize=12)\n",
        "axes[1, 1].set_title('Precision Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x_pos = np.arange(len(comparison_df))\n",
        "width = 0.35\n",
        "\n",
        "cv_means = comparison_df['PR-AUC (CV Mean)'].values\n",
        "cv_stds = comparison_df['PR-AUC (CV Std)'].values\n",
        "\n",
        "ax.barh(x_pos, cv_means, width, xerr=cv_stds, label='CV Mean ± Std', color='steelblue', alpha=0.7)\n",
        "ax.set_yticks(x_pos)\n",
        "ax.set_yticklabels(comparison_df['Model'])\n",
        "ax.set_xlabel('PR-AUC Score', fontsize=12)\n",
        "ax.set_title('Cross-Validation PR-AUC Comparison (Mean ± Std)', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model based on PR-AUC (most important metric for imbalanced data)\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_pr_auc = comparison_df.iloc[0]['PR-AUC (Test)']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL SELECTION DECISION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSelected Best Model: {best_model_name}\")\n",
        "print(f\"Test Set PR-AUC: {best_pr_auc:.4f}\")\n",
        "print(f\"Test Set F1-Score: {comparison_df.iloc[0]['F1-Score (Test)']:.4f}\")\n",
        "print(f\"Test Set Recall: {comparison_df.iloc[0]['Recall (Test)']:.4f}\")\n",
        "print(f\"Test Set Precision: {comparison_df.iloc[0]['Precision (Test)']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"JUSTIFICATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "Selection Criteria:\n",
        "1. PR-AUC (Primary): Most important metric for imbalanced data, measures \n",
        "   precision-recall trade-off across all thresholds\n",
        "2. F1-Score: Harmonic mean of precision and recall, balances both metrics\n",
        "3. Recall: Critical for fraud detection - we want to catch as many fraud cases as possible\n",
        "4. Cross-Validation Stability: Low standard deviation indicates consistent performance\n",
        "5. Interpretability: Important for business stakeholders and regulatory compliance\n",
        "\n",
        "Rationale:\n",
        "- PR-AUC is preferred over ROC-AUC for imbalanced datasets because it focuses on \n",
        "  the positive class (fraud) which is our primary concern\n",
        "- High recall is crucial to minimize false negatives (missed fraud cases)\n",
        "- Model should show consistent performance across cross-validation folds\n",
        "- Balance between performance and interpretability for business use\n",
        "\"\"\")\n",
        "\n",
        "# Get the actual best model object\n",
        "if 'XGBoost (Tuned)' in best_model_name:\n",
        "    best_model = xgb_tuned.best_estimator_\n",
        "elif 'XGBoost' in best_model_name and 'Tuned' not in best_model_name:\n",
        "    best_model = xgb_model\n",
        "elif 'Random Forest' in best_model_name:\n",
        "    best_model = rf_model\n",
        "elif 'LightGBM' in best_model_name:\n",
        "    best_model = lgb_model\n",
        "else:\n",
        "    best_model = lr_model\n",
        "\n",
        "print(f\"\\nBest model object: {type(best_model).__name__}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Best Model\n",
        "\n",
        "Save the selected best model and preprocessing objects for deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory if it doesn't exist\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save best model\n",
        "model_filename = models_dir / f'best_model_{best_model_name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}.pkl'\n",
        "joblib.dump(best_model, model_filename)\n",
        "print(f\"Best model saved to: {model_filename}\")\n",
        "\n",
        "# Also save all models for comparison\n",
        "joblib.dump(lr_model, models_dir / 'logistic_regression.pkl')\n",
        "joblib.dump(rf_model, models_dir / 'random_forest.pkl')\n",
        "joblib.dump(xgb_model, models_dir / 'xgboost.pkl')\n",
        "joblib.dump(lgb_model, models_dir / 'lightgbm.pkl')\n",
        "if 'xgb_tuned' in locals():\n",
        "    joblib.dump(xgb_tuned.best_estimator_, models_dir / 'xgboost_tuned.pkl')\n",
        "\n",
        "print(\"\\nAll models saved successfully!\")\n",
        "\n",
        "# Save comparison results\n",
        "comparison_df.to_csv(models_dir / 'model_comparison_results.csv', index=False)\n",
        "print(f\"Model comparison results saved to: {models_dir / 'model_comparison_results.csv'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TASK 2 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Model saved to: {model_filename}\")\n",
        "print(f\"\\nNext Steps:\")\n",
        "print(\"1. Proceed to Task 3: Model Explainability using SHAP\")\n",
        "print(\"2. Analyze feature importance from the best model\")\n",
        "print(\"3. Generate explainability visualizations\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
